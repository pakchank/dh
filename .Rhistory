library(tidyverse)
library(purrr)
library(tidytext)
library(KoNLP)
library(readxl)
library(httr)
library(jsonlite)
library(shiny)
url <- "https://koddas.knu.ac.kr/api/v1/statement"
key <- "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImFwaXVzZXIiLCJpYXQiOjE3MzUyOTc0MDIsImV4cCI6MTc2NjgzMzQwMn0.q9upgIiYB6haOzc-aUUlwXk5qKvydFZtmDVzXarI1VI"
getPara <- function(page, book){
res <- GET(url=url,
query=list(key=key,
page=page,
limit=100,
bookNumber=book))
dtString <- content(res, type="text", encoding="utf-8")
dt <- fromJSON(dtString)
return(dt)
}
list_big <- list()
page <- 1
i <- 1
for (book in 28:31) {
while (TRUE) {
dt <- getPara(page=page, book=book)
if (length(dt) == 0) {
break
}
list_big[[i]] <- dt
page <- page + 1
i <- i + 1
}
page <- 1
}
df <- bind_rows(list_big)
df <- df |>
filter(nchar(content) > 100)
ner <- function(text) {
body = paste0('{"argument": {"text": "', text, '", "analysis_code": "ner"}}')
res <- POST(url="http://aiopen.etri.re.kr:8000/WiseNLU_spoken",
body= body,
encode="raw",
add_headers(.headers=c("Content-Type" = "application/json; charset=UTF-8",
"Authorization"="45a48207-93d4-4d40-9b10-1db71ea2b3ce")))
sentences <- fromJSON(content(res, as="text", encoding='UTF-8'), simplifyVector=FALSE)$return_object$sentence
out <- 1:length(sentences) |>
map(~sentences[[.x]]$NE) |>
map(~bind_rows(.x)) |>
bind_rows(.id="sentence")
return(out)
}
ner_para <- function(row){
#print(row$TEXT)
nered <- ner(row$TEXT)
nered$para <- row$para
nered$book <- row$book
return(nered)
}
list_large <- list()
for (i in 1:length(nrow(df))){
list_large[[i]] <- ner_para(df[i, ])
}
ner <- function(text) {
body = paste0('{"argument": {"text": "', text, '", "analysis_code": "ner"}}')
res <- POST(url="http://aiopen.etri.re.kr:8000/WiseNLU_spoken",
body= body,
encode="raw",
add_headers(.headers=c("Content-Type" = "application/json; charset=UTF-8",
"Authorization"="45a48207-93d4-4d40-9b10-1db71ea2b3ce")))
sentences <- fromJSON(content(res, as="text", encoding='UTF-8'), simplifyVector=FALSE)$return_object$sentence
print(sentences)
out <- 1:length(sentences) |>
map(~sentences[[.x]]$NE) |>
map(~bind_rows(.x)) |>
bind_rows(.id="sentence")
return(out)
}
ner_para <- function(row){
#print(row$TEXT)
nered <- ner(row$TEXT)
nered$para <- row$para
nered$book <- row$book
return(nered)
}
list_large <- list()
for (i in 1:length(nrow(df))){
list_large[[i]] <- ner_para(df[i, ])
}
df
View(df)
df <- df |>
filter(nchar(content) > 100) |>
group_by(bookId) |>
mutate(para=row_number()) |>
ungroup()
ner <- function(text) {
body = paste0('{"argument": {"text": "', text, '", "analysis_code": "ner"}}')
res <- POST(url="http://aiopen.etri.re.kr:8000/WiseNLU_spoken",
body= body,
encode="raw",
add_headers(.headers=c("Content-Type" = "application/json; charset=UTF-8",
"Authorization"="45a48207-93d4-4d40-9b10-1db71ea2b3ce")))
sentences <- fromJSON(content(res, as="text", encoding='UTF-8'), simplifyVector=FALSE)$return_object$sentence
#print(sentences)
out <- 1:length(sentences) |>
map(~sentences[[.x]]$NE) |>
map(~bind_rows(.x)) |>
bind_rows(.id="sentence")
return(out)
}
ner_para <- function(row){
#print(row$TEXT)
nered <- ner(row$content) # 데이터 변수명에 따라 이 부분 바꾸어주어야 함.
nered$para <- row$para
nered$book <- row$bookId
return(nered)
}
list_large <- list()
for (i in 1:length(nrow(df))){
list_large[[i]] <- ner_para(df[i, ])
}
list_large <- list()
for (i in 1:nrow(df)){
list_large[[i]] <- ner_para(df[i, ])
}
dt_ner <- bind_rows(list_large)
saveRDS(dt_ner, "dt_ner.rds")
View(dt_ner)
df[1194,] |> View()
df[1193,] |> View()
df[1195,] |> View()
library(tidyverse)
library(purrr)
library(tidytext)
library(tidygraph)
library(igraph)
library(networkD3)
library(widyr)
library(ggraph)
library(glue)
library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()
dt_ner <- readRDS("dt_ner.rds")
dt_ner |>
filter(!str_detect(type, "DT_|QT_|AM_|TI_|MT_|PT_")) |>
#!(type %in% c("")))
count(text, sort=T)
dt_cat <- dt_ner |>
filter(str_detect(type, "(^PS_NAME)|^(LC)|^(OG)")) |>
mutate(cat = case_when(str_detect(type, "^PS_NAME") ~  "인물",
str_detect(type, "^LC") ~ "장소",
str_detect(type, "^OG") ~ "조직"))
nodeAttr <- dt_cat |>
group_by(text, cat) |>
summarise(text=first(text), cat=first(cat)) |>
ungroup()
dupList <- nodeAttr |>
group_by(text) |>
summarise(n = n()) |>
filter(n >= 2) |>
pull(text)
nodeAttr |>
filter(text %in% dupList)
g <- dt_cat |>
filter(!(text %in% dupList)) |>
unite("bp", c("book", "para")) |>
pairwise_count(text, bp, sort=TRUE, diag=FALSE) |>
rename(weight=n) |>
#filter(weight > 5) |>
graph_from_data_frame(directed=FALSE)
is_weighted(g)
gTidy <- g |>
as_tbl_graph()
saveRDS(gTidy, "gTidy.rds")
gTidy |>
activate(edges) |>
filter(weight > 4) |>
activate(nodes) |>
left_join(nodeAttr |>
filter(!(text %in% dupList)), by=c("name"="text")) |>
mutate(degree = centrality_degree()) |>
filter(degree != 0) |>
ggraph() +
aes(edgh_width=weight, alpah=weight) +
geom_edge_link(alpha = 0.50, edge_color = "grey20") +
geom_node_point(aes(color=cat), size=10) +
#  geom_node_text(aes(label=name)) |>
geom_node_text(aes(label = name),         # 텍스트 표시
repel = T,                 # 노드밖 표시
size = 5,                  # 텍스트 크기
family = "nanumgothic") +  # 폰트
theme_graph()                             # 배경 삭제                       # 배경 삭제
pair <- dt_ner |>
filter(!str_detect(type, "DT_|QT_|AM_|TI_|MT_|PT_")) |>
pairwise_count(item = text,
feature = para,    # 다른 책의 같은 para일 수 있음 -> book도 같이 구분하도록 고쳐야 함!
sort = T)
pair |>
filter(item1 == "조선족")
graph_kd <- pair %>%
filter(n >= 10) %>%
as_tbl_graph(directed = F) %>%
mutate(centrality = centrality_degree(),    # 연결 중심성
group = as.factor(group_infomap()))  # 커뮤니티
graph_kd |>
ggraph(layout = "fr") +      # 레이아웃
geom_edge_link(color = "gray50",          # 엣지 색깔
alpha = 0.5) +             # 엣지 명암
geom_node_point(aes(size = centrality,    # 노드 크기
color = group),       # 노드 색깔
show.legend = F) +        # 범례 삭제
scale_size(range = c(5, 15)) +            # 노드 크기 범위
geom_node_text(aes(label = name),         # 텍스트 표시
repel = T,                 # 노드밖 표시
size = 5,                  # 텍스트 크기
family = "nanumgothic") +  # 폰트
theme_graph()                             # 배경 삭제
gig <- gTidy |>
activate(edges) |>
filter(weight > 4) |>
activate(nodes) |>
left_join(nodeAttr |>
filter(!(text %in% dupList)), by=c("name"="text")) |>
mutate(degree = centrality_degree()) |>
filter(degree != 0) |>
as.igraph()
gD3 <- gig |>
igraph_to_networkD3(group=vertex_attr(gig)$cat)
forceNetwork(Links=gD3$links, Nodes=gD3$nodes,
Source = 'source', Target = 'target', NodeID = 'name', Group = 'group')
