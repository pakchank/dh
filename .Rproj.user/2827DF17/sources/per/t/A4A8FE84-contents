# Tidy한 텍스트 분석


```{r}
library(tidyverse)
library(purrr)
library(tidytext)
library(KoNLP)
library(readxl)
library(httr)
library(jsonlite)
library(shiny)
```


## API를 이용한 K-디아스포라 DB 이용
### API 테스트
```{r}
res <- GET(url="https://koddas.knu.ac.kr/api/v1/statement",
           query=list(key="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImFwaXVzZXIiLCJpYXQiOjE3MzUyOTc0MDIsImV4cCI6MTc2NjgzMzQwMn0.q9upgIiYB6haOzc-aUUlwXk5qKvydFZtmDVzXarI1VI",
                      page=7,
                      limit="100",
                      bookNumber="28"))

dtString <- content(res, type="text", encoding="utf-8")

dt <- fromJSON(dtString)
```

### 함수 만들기
```{r}
url <- "https://koddas.knu.ac.kr/api/v1/statement"
key <- "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImFwaXVzZXIiLCJpYXQiOjE3MzUyOTc0MDIsImV4cCI6MTc2NjgzMzQwMn0.q9upgIiYB6haOzc-aUUlwXk5qKvydFZtmDVzXarI1VI"

getPara <- function(page, book){
    res <- GET(url=url,
               query=list(key=key,
                          page=page,
                          limit=100,
                          bookNumber=book))
    dtString <- content(res, type="text", encoding="utf-8")

    dt <- fromJSON(dtString)    

    return(dt)
    }
```

```{r}
list_big <- list()
page <- 1
i <- 1
    
for (book in 28:31) {     
    while (TRUE) {
        dt <- getPara(page=page, book=book)
        
        if (length(dt) == 0) {
            break
        }
        list_big[[i]] <- dt
        page <- page + 1
        i <- i + 1
    }
    page <- 1
}
```

```{r}
df <- bind_rows(list_big)
```

```{r}
df |>
    filter(nchar(content) > 100)
```


```{r}
dt <- getPara(1,31)
```


## 로컬 텍스트 파일 부르기

```{r}
filepath <- "5. 총서 텍스트DB 최종/"
dt <- paste0(filepath, list.files(filepath)) |>
    set_names() |>
    map(~read_excel(.)) |>
    map(~rename(., "para" = 1)) |>
    map(~mutate(., para=as.character(para))) |>
    bind_rows(.id="book") |>
    mutate(book = str_replace(book, ".*/book", ""),
           book = str_replace(book, "( 최종.xlsm)|( 최종.xlsx)|( 최종\\(일부 수정\\).xlsm)", ""),
           book = as.numeric(book)) |> 
    filter(book >= 28)                               # 28권 이후 공개
```
, `Person:Period;`, Category_num, Category2, Keywords, 
           `subject-predicate-object;`, Keywords_paper, `Category-num`
  
           
## Tidytext와 KoNLP를 이용한 형태소 분석 방법.
```{r}
tidydt <- dt |>
    select(book, para, TEXT) |>
    unnest_tokens(word, TEXT, token=SimplePos22)
beep()
```

```{r}
tidydt <- df |>
```

```{r}
tidydt |>
    mutate(word = str_replace(word, "\\+(.*)", "")) |>
    filter(str_detect(word, "/nc|/nq")) |>
    separate_wider_delim(word, delim="/", names=c("word", "morph"), too_many="debug") |>
    filter(!word_ok)
```

```{r}
tidydt |>
    mutate(word = str_replace(word, "\\+(.*)", "")) |>
    filter(str_detect(word, "/nc|/nq")) |>
    separate_wider_delim(word, delim="/", names=c("word", "morph"), too_many="drop") |>
    filter(morph!="nc")
```






## ETRI NER을 이용한 방법.
- 이용자 사전을 이용한 형태소 분석 등은 하지 않았음.
```{r}
ner <- function(text) {
    body = paste0('{"argument": {"text": "', text, '", "analysis_code": "ner"}}')
    res <- POST(url="http://aiopen.etri.re.kr:8000/WiseNLU_spoken",
            body= body,
            encode="raw",
            add_headers(.headers=c("Content-Type" = "application/json; charset=UTF-8",
                                   "Authorization"="45a48207-93d4-4d40-9b10-1db71ea2b3ce"))) 
    
    sentences <- fromJSON(content(res, as="text", encoding='UTF-8'), simplifyVector=FALSE)$return_object$sentence
    
    out <- 1:length(sentences) |>
        map(~sentences[[.x]]$NE) |>
        map(~bind_rows(.x)) |>
        bind_rows(.id="sentence")
    
    return(out)
}

ner_para <- function(row){
    #print(row$TEXT)
    nered <- ner(row$TEXT)
    nered$para <- row$para
    nered$book <- row$book
    
    return(nered)
}
```

```{r}
list_large <- list()

for (i in 1:1000){
    list_large[[i]] <- ner_para(dt[i, ])
}
```



```{r}
dt_ner <- bind_rows(list_large)

```



```{r}
#saveRDS(dt_ner, "dt_ner.rds")
dt_ner <- readRDS("dt_ner.rds")
```