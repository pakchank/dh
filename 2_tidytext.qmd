# Tidy한 텍스트 분석

```{r eval=FALSE}
library(tidyverse)
library(purrr)
library(tidytext)
library(KoNLP)
library(readxl)
library(httr)
library(jsonlite)
library(shiny)
```

## API를 이용한 K-디아스포라 DB 이용

### K-디아스포라 API 테스트

```{r eval=FALSE}
res <- GET(url="https://koddas.knu.ac.kr/api/v1/statement",
           query=list(key="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImFwaXVzZXIiLCJpYXQiOjE3MzUyOTc0MDIsImV4cCI6MTc2NjgzMzQwMn0.q9upgIiYB6haOzc-aUUlwXk5qKvydFZtmDVzXarI1VI",
                      page=7,
                      limit="2",
                      bookNumber="28"))

dtString <- content(res, type="text", encoding="utf-8")

dt <- fromJSON(dtString)
```

### 함수 만들기
K-디아스포라 API로부터 반복적으로 데이터를 불러오기 위한 함수입니다.
```{r eval=FALSE}
url <- "https://koddas.knu.ac.kr/api/v1/statement"
key <- "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImFwaXVzZXIiLCJpYXQiOjE3MzUyOTc0MDIsImV4cCI6MTc2NjgzMzQwMn0.q9upgIiYB6haOzc-aUUlwXk5qKvydFZtmDVzXarI1VI"

getPara <- function(page, book){
    res <- GET(url=url,
               query=list(key=key,
                          page=page,
                          limit=100,
                          bookNumber=book))
    dtString <- content(res, type="text", encoding="utf-8")

    dt <- fromJSON(dtString)    

    return(dt)
    }
```

28권부터 31권까지 불러와서 `list`로 저장.
```{r eval=FALSE}
list_big <- list()
page <- 1
i <- 1
    
for (book in 28:31) {     
    while (TRUE) {
        dt <- getPara(page=page, book=book)
        
        if (length(dt) == 0) {
            break
        }
        list_big[[i]] <- dt
        page <- page + 1
        i <- i + 1
    }
    page <- 1
}
```

`list_big` 안에 저장된 여러 테이블을 결합.
```{r eval=FALSE}
df <- bind_rows(list_big)
```

```{r eval=FALSE}
df |>
    filter(nchar(content) > 100)
```

```{r}
#dt <- getPara(1,31)
```


## 로컬 텍스트 파일 부르기 (API가 작동하지 않는 경우에만 이용)

```{r eval=FALSE}
filepath <- "5. 총서 텍스트DB 최종/"
dt <- paste0(filepath, list.files(filepath)) |>
    set_names() |>
    map(~read_excel(.)) |>
    map(~rename(., "para" = 1)) |>
    map(~mutate(., para=as.character(para))) |>
    bind_rows(.id="book") |>
    mutate(book = str_replace(book, ".*/book", ""),
           book = str_replace(book, "( 최종.xlsm)|( 최종.xlsx)|( 최종\\(일부 수정\\).xlsm)", ""),
           book = as.numeric(book)) |> 
    filter(book >= 28)                               # 28권 이후 공개
```

## Tidytext와 KoNLP를 이용한 형태소 분석 방법.

```{r eval=FALSE}
tidydt <- dt |>
    select(book, para, TEXT) |>
    unnest_tokens(word, TEXT, token=SimplePos22)
beep()
```

```{r eval=FALSE}
tidydt |>
    mutate(word = str_replace(word, "\\+(.*)", "")) |>
    filter(str_detect(word, "/nc|/nq")) |>
    separate_wider_delim(word, delim="/", names=c("word", "morph"), too_many="drop") |>
    filter(morph!="nc")
```


## ETRI NER을 이용한 방법.

```{r eval=FALSE}
ner <- function(text) {
    body = paste0('{"argument": {"text": "', text, '", "analysis_code": "ner"}}')
    res <- POST(url="http://aiopen.etri.re.kr:8000/WiseNLU_spoken",
            body= body,
            encode="raw",
            add_headers(.headers=c("Content-Type" = "application/json; charset=UTF-8",
                                   "Authorization"="45a48207-93d4-4d40-9b10-1db71ea2b3ce"))) 
    
    sentences <- fromJSON(content(res, as="text", encoding='UTF-8'), simplifyVector=FALSE)$return_object$sentence
    
    out <- 1:length(sentences) |>
        map(~sentences[[.x]]$NE) |>
        map(~bind_rows(.x)) |>
        bind_rows(.id="sentence")
    
    return(out)
}

ner_para <- function(row){
    #print(row$TEXT)
    nered <- ner(row$TEXT)
    nered$para <- row$para
    nered$book <- row$book
    
    return(nered)
}
```

```{r eval=FALSE}
list_large <- list()

for (i in 1:1000){
    list_large[[i]] <- ner_para(dt[i, ])
}
```

```{r eval=FALSE}
dt_ner <- bind_rows(list_large)
```

NER 분석 결과물을 로컬 파일로 저장.
```{r eval=FALSE}
#saveRDS(dt_ner, "dt_ner.rds")
#dt_ner <- readRDS("dt_ner.rds")
```
